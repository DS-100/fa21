{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:15.363920Z",
     "start_time": "2018-02-02T15:15:14.337886Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('county_and_state.csv') as f:\n",
    "    county_and_state = pd.read_csv(f)\n",
    "    \n",
    "with open('county_and_population.csv') as f:\n",
    "    county_and_pop = pd.read_csv(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we'd like to join these two tables. Unfortunately, we can't, because the strings representing the county names don't match, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before we can join them, we'll do what I call **canonicalization**.\n",
    "\n",
    "Canonicalization: A process for converting data that has more than one possible representation into a \"standard\", \"normal\", or canonical form (definition via Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_county(county_name):\n",
    "    return (\n",
    "        county_name\n",
    "        .lower()               # lower case\n",
    "        .replace(' ', '')      # remove spaces\n",
    "        .replace('&', 'and')   # replace &\n",
    "        .replace('.', '')      # remove dot\n",
    "        .replace('county', '') # remove county\n",
    "        .replace('parish', '') # remove parish\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop['clean_county'] = county_and_pop['County'].map(canonicalize_county)\n",
    "county_and_state['clean_county'] = county_and_state['County'].map(canonicalize_county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop.merge(county_and_state,\n",
    "                     left_on = 'clean_county', right_on = 'clean_county')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data from a Text Log Using Basic Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('log.txt', 'r') as f:\n",
    "    log_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to extract the day, month, year, hour, minutes, seconds, and timezone. Looking at the data, we see that these items are not in a fixed position relative to the beginning of the string. That is, slicing by some fixed offset isn't going to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[0][20:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[1][20:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll need to use some more sophisticated thinking. Let's focus on only the first line of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = log_lines[0]\n",
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertinent = first.split(\"[\")[1].split(']')[0]\n",
    "day, month, rest = pertinent.split('/')\n",
    "year, hour, minute, rest = rest.split(':')\n",
    "seconds, time_zone = rest.split(' ')\n",
    "day, month, year, hour, minute, seconds, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much more sophisticated but common approach is to extract the information we need using a regular expression. See [today's lecture slides](https://docs.google.com/presentation/d/1omFKPsCaPf58VLo33U9ipRq4vt1Kj_3y9yHDUl8qYhk/edit#slide=id.g41d831233f_1_119) for more on regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'\\[(\\d+)/(\\w+)/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "day, month, year, hour, minute, second, time_zone = re.search(pattern, first).groups()\n",
    "year, month, day, hour, minute, second, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternately using the `findall` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'\\[(\\d+)/(\\w+)/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "day, month, year, hour, minute, second, time_zone = re.findall(pattern, first)[0]\n",
    "year, month, day, hour, minute, second, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can return the results as a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Day\", \"Month\", \"Year\", \"Hour\", \"Minute\", \"Second\", \"Time Zone\"]\n",
    "def log_entry_to_series(line):\n",
    "    return pd.Series(re.search(pattern, line).groups(), index = cols)\n",
    "\n",
    "log_entry_to_series(first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using this function we can create a DataFrame of all the time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_info = pd.DataFrame(columns=cols)\n",
    "\n",
    "for line in log_lines:\n",
    "    log_info = log_info.append(log_entry_to_series(line), ignore_index = True)\n",
    "\n",
    "log_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression From Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the regex below so that after code executes, day is “26”, month is “Jan”, and year is “2014”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = r\"YOUR REGEX HERE\"\n",
    "matches = re.findall(pattern, log_lines[0])\n",
    "#day, month, year = matches[0]\n",
    "#day, month, year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Example #1: Restaurant Data\n",
    "\n",
    "In this example, we will show how regexes can allow us to track quantitative data across categories defined by the appearance of various text fields.\n",
    "\n",
    "In this example we'll see how the presence of certain keywords can affect quantitative data, e.g. how do restaurant health scores vary as a function of the number of violations that mention \"vermin\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio = pd.read_csv('violations.csv', header=0, names=['id', 'date', 'desc'])\n",
    "desc = vio['desc']\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = desc.value_counts()\n",
    "\n",
    "counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hmmm...\n",
    "counts[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use regular expressions to cut out the extra info in square braces.\n",
    "vio['clean_desc'] = (vio['desc']\n",
    "             .str.replace('\\s*\\[.*\\]$', '')\n",
    "             .str.strip()\n",
    "             .str.lower())\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['clean_desc'].value_counts().head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use regular expressions to assign new features for the presence of various keywords\n",
    "with_features = (vio\n",
    " .assign(is_clean     = vio['clean_desc'].str.contains('clean|sanit'))\n",
    " .assign(is_high_risk = vio['clean_desc'].str.contains('high risk'))\n",
    " .assign(is_vermin    = vio['clean_desc'].str.contains('vermin'))\n",
    " .assign(is_surface   = vio['clean_desc'].str.contains('wall|ceiling|floor|surface'))\n",
    " .assign(is_human     = vio['clean_desc'].str.contains('hand|glove|hair|nail'))\n",
    " .assign(is_permit    = vio['clean_desc'].str.contains('permit|certif'))\n",
    ")\n",
    "with_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features = (with_features\n",
    " .groupby(['id', 'date'])\n",
    " .sum()\n",
    " .reset_index()\n",
    ")\n",
    "count_features.iloc[255:260, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features.query('is_vermin > 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a new pandas feature called \"melt\" that we won't describe in any detail\n",
    "#the granularity of the resulting frame is a violation type in a given inspection\n",
    "broken_down_by_violation_type = pd.melt(count_features, id_vars=['id', 'date'],\n",
    "            var_name='feature', value_name='num_vios')\n",
    "broken_down_by_violation_type.sort_values([\"id\", \"date\"]).head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the scores\n",
    "ins = pd.read_csv('inspections.csv',\n",
    "                  header=0,\n",
    "                  usecols=[0, 1, 2],\n",
    "                  names=['id', 'score', 'date'])\n",
    "ins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join scores with the table broken down by violation type\n",
    "violation_type_and_scores = (\n",
    "    broken_down_by_violation_type\n",
    "    .merge(ins, left_on=['id', 'date'], right_on=['id', 'date'])\n",
    ")\n",
    "violation_type_and_scores.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='num_vios', y='score',\n",
    "               col='feature', col_wrap=2,\n",
    "               kind='box',\n",
    "               data=violation_type_and_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see, for example, that if a restaurant inspection involved 2 violation with the keyword \"vermin\", the average score for that inspection would be a little bit below 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Example 2: Police Data\n",
    "\n",
    "In this example, we will apply string processing to the process of data cleaning and exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data\n",
    "\n",
    "The city of Berkeley maintains an [Open Data Portal](https://data.cityofberkeley.info/) for citizens to access data about the city.  We will be examining [Call Data](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Calls-for-Service/k2nh-s5h5).\n",
    "\n",
    "<img src=\"calls_desc.png\" width=800px />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.115040Z",
     "start_time": "2018-02-02T15:15:19.070620Z"
    }
   },
   "outputs": [],
   "source": [
    "import ds100_utils\n",
    "\n",
    "calls_url = 'https://data.cityofberkeley.info/api/views/k2nh-s5h5/rows.csv?accessType=DOWNLOAD'\n",
    "calls_file = ds100_utils.fetch_and_cache(calls_url, 'calls.csv')\n",
    "calls = pd.read_csv(calls_file, warn_bad_lines=True)\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.122345Z",
     "start_time": "2018-02-02T15:15:19.118071Z"
    }
   },
   "outputs": [],
   "source": [
    "len(calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an example `Block_Location` value look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calls['Block_Location'].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Preliminary observations on the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "1. `EVENTDT` -- Contains the incorrect time\n",
    "1. `EVENTTM` -- Contains the time in 24 hour format (What timezone?)\n",
    "1. `CVDOW` -- Encodes the day of the week (see data documentation).\n",
    "1. `InDbDate` -- Appears to be correctly formatted and appears pretty consistent in time.\n",
    "1. **`Block_Location` -- a multi-line string that contains coordinates.**\n",
    "1. `BLKADDR` -- Appears to be the address in `Block Location`.\n",
    "1. `City` and `State` seem redundant given this is supposed to be the city of Berkeley dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting locations\n",
    "\n",
    "The block location contains geographic coordinates. Let's extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.230720Z",
     "start_time": "2018-02-02T15:15:19.225971Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['Block_Location'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.268222Z",
     "start_time": "2018-02-02T15:15:19.233193Z"
    }
   },
   "outputs": [],
   "source": [
    "calls_lat_lon = (\n",
    "    calls['Block_Location']\n",
    "    .str.extract(\"\\((\\d+\\.\\d+)\\, (-\\d+\\.\\d+)\\)\")\n",
    ")\n",
    "calls_lat_lon.columns = ['Lat', 'Lon']\n",
    "calls_lat_lon.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.280471Z",
     "start_time": "2018-02-02T15:15:19.271307Z"
    }
   },
   "outputs": [],
   "source": [
    "calls_lat_lon.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls[calls_lat_lon.isnull().any(axis=1)]['Block_Location'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join in the extracted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.328544Z",
     "start_time": "2018-02-02T15:15:19.287625Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calls['Lat'] = calls_lat_lon['Lat']\n",
    "calls['Lon'] = calls_lat_lon['Lon']\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Location information\n",
    "\n",
    "Let's examine the geographic data (latitude and longitude).  Recall that we had some missing values.  Let's look at the behavior of these missing values according to crime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:34.130428Z",
     "start_time": "2018-02-02T15:15:32.851717Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_lat_lon = calls[calls[['Lat', 'Lon']].isnull().any(axis=1)]\n",
    "missing_lat_lon['CVLEGEND'].value_counts().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls['CVLEGEND'].value_counts().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might further normalize the analysis by the frequency to find which type of crime has the highest proportion of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:35.358758Z",
     "start_time": "2018-02-02T15:15:34.132788Z"
    }
   },
   "outputs": [],
   "source": [
    "(missing_lat_lon['CVLEGEND'].value_counts() \n",
    " / calls['CVLEGEND'].value_counts()\n",
    ").sort_values(ascending=False).plot(kind=\"barh\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a crime map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to install the folium package for this to work\n",
    "# to do this, uncomment the line below and run it.\n",
    "# !pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import folium.plugins\n",
    "\n",
    "SF_COORDINATES = (37.87, -122.28)\n",
    "sf_map = folium.Map(location=SF_COORDINATES, zoom_start=13)\n",
    "locs = calls[['Lat', 'Lon']].astype('float').dropna().values\n",
    "heatmap = folium.plugins.HeatMap(locs.tolist(), radius=10)\n",
    "sf_map.add_child(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Is campus really the safest place to be?\n",
    "1. Why are all the calls located on the street and at often at intersections?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:54.826669Z",
     "start_time": "2018-02-02T15:15:37.180513Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import folium.plugins\n",
    "\n",
    "locations = calls[calls['CVLEGEND'] == 'ASSAULT'][['Lat', 'Lon']]\n",
    "\n",
    "cluster = folium.plugins.MarkerCluster()\n",
    "for _, r in locations.dropna().iterrows():\n",
    "    cluster.add_child(\n",
    "        folium.Marker([float(r[\"Lat\"]), float(r[\"Lon\"])]))\n",
    "    \n",
    "sf_map = folium.Map(location=SF_COORDINATES, zoom_start=13)\n",
    "sf_map.add_child(cluster)\n",
    "sf_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Content: Using pd.to_datetime to Extract Time Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date parsing using `pd.to_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(log_lines).str.extract(r'\\[(.*) -0800\\]').apply(\n",
    "    lambda s: pd.to_datetime(s, format='%d/%b/%Y:%H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
