---
layout: page
title: Lecture 14 – Ordinary Least Squares
nav_exclude: true
---

# Lecture 14 – Ordinary Least Squares

Presented by Anthony D. Joseph and Suraj Rampure

Content by Suraj Rampure, Ani Adhikari, Deb Nolan, Joseph Gonzalez

- [slides](https://docs.google.com/presentation/d/1aeNPAe0g2C4CfU-oxjnnd66fKri0MYtGKYTizlouuec/edit?usp=sharing)
- [video playlist](https://youtube.com/playlist?list=PLQCcNQgUcDfrVbm9gLBCY6LEj52m1Bbdy)
- [code](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/fa21&subPath=lec/lec14/)

A reminder – the right column of the table below contains _Quick Checks_. These are **not** required but suggested to help you check your understanding.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>14.1</strong> <br />A quick recap of the modeling process, and a roadmap for lecture.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/HS8W2dl5KXo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/A9tQ7K2xTe5cy5Zm7" target="\_blank">14.1</a></td>
</tr>
<tr>
<td><strong>14.2</strong> <br />Defining the multiple linear regression model using linear algebra (dot products and matrix multiplication). Introducing the idea of a design matrix.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/oGIPhLtVb6k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/hcgKbrJvHXHUAQha7" target="\_blank">14.2</a></td>
</tr>
<tr>
<td><strong>14.3</strong> <br />Defining the mean squared error of the multiple linear regression model as the (scaled) norm of the residual vector.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/odY5eSwJ02w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/L8WqsEyCZRtESY4N9" target="\_blank">14.3</a></td>
</tr>
<tr>
<td><strong>14.4</strong> <br />Using a geometric argument to determine the optimal model parameter.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/nkLUTatnK0s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/bhmiDSLDdJDbigMw8" target="\_blank">14.4</a></td>
</tr>
<tr>
<td><strong>14.5</strong> <br />Residual plots. Properties of residuals, with and without an intercept term in our model.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/lT_gzva-dKg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/YkSWw7iR6vJzXfZA6" target="\_blank">14.5</a></td>
</tr>
<tr>
<td><strong>14.6</strong> <br />Discussing the conditions in which there isn't a unique solution for the optimal model parameter. A summary, and outline of what is to come.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/9e_w8up-8Yc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/z6hNshYCtLB4biHf6" target="\_blank">14.6</a></td>
</tr>
</tbody>
</table>
