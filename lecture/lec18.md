---
layout: page
title: Lecture 18 – Gradient Descent
nav_exclude: true
---

# Lecture 18 – Gradient Descent

Presented by Raguvir Kunani, Anthony D. Joseph

Content by Raguvir Kunani, Josh Hug, Joseph Gonzalez, Paul Shao

- [slides](https://docs.google.com/presentation/d/1YAymtiN92ipgIw9MEXFCIoEHP5csdoeX0_vVo37LLXo/edit?usp=sharing)
- [video playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfpuAxccmVe2yu19U-kmLy0j)
- [code](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/su21&subPath=lec/lec18/&branch=main)
- [code HTML](../../resources/assets/lectures/lec18/lec18.html)
- (optional) [Raguvir's Gradient Descent Walkthrough](https://raguvir.me/teaching/ds100/resources/other/gradient_descent_walkthrough.pdf)
- (optional) [Loss Game](../../resources/assets/lectures/lec18/gradient_game_v3.html)

A reminder – the right column of the table below contains _Quick Checks_. These are **not** required but suggested to help you check your understanding.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>18.0</strong> <br>Introduction and Motivating Gradient Descent.</td>
<td><iframe width="300" height="300" height src="https://www.youtube.com/embed/K7xinj5-8Oo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td></td>
</tr>
<tr>
<td><strong>18.1</strong> <br>Gradient descent in one dimension. Convexity.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/gQq93hzecHM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSeoSD6AtUlyRrf9hCQWDwGn8PqL9SdWU479C_Q_P8CJ5F45Lg/viewform" target="\_blank">18.1</a></td>
</tr>
<tr>
<td><strong>18.2</strong> <br>Various methods of optimizing loss functions in one dimension.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/AzxMoqcyWzI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfsx1xVVxT8aeK6H1Qx_hPzHkOwPP1xncK3U3Ornt8vpLKIdQ/viewform" target="\_blank">18.2</a></td>
</tr>
<tr>
<td><strong>18.3</strong> <br>Gradient descent in multiple dimensions. Interpretation of gradients.</td>
<td><iframe width="300" height="500" height src="https://youtube.com/embed/16nIdtc5x9k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLScFeTRKsws3FlqlcAYLqiPlOn98nNbjJZKADpsrgYk-ZOGwmA/viewform" target="\_blank">18.3</a></td>
</tr>
<tr>
<td><strong>18.4</strong> <br>Stochastic gradient descent (SGD). Comparison between gradient descent and SGD.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/CWaZS14cdh8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfveRPyYF25Pp6vrehPumB6M7kRJjzJhCUBoCoZ_TfbuxzruA/viewform" target="\_blank">18.4</a></td>
</tr>
